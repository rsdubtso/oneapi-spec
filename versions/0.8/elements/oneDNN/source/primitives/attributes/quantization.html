

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quantization &mdash; oneAPI Specification 0.8 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../../../_static/favicons.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/language_data.js"></script>
        <script src="../../../../../_static/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="Post-ops" href="post-ops.html" />
    <link rel="prev" title="Scratchpad Mode" href="scratchpad.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="https://oneapi.com" class="icon icon-home"> oneAPI Specification
          

          
            
            <img src="../../../../../_static/oneAPI-rgb-rev-100.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../architecture.html">Software Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../library-interop.html">Library Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../elements.html">oneAPI Elements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dpcpp/source/index.html">DPC++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneDPL/source/index.html">oneDPL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">oneDNN</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../conventions.html">Conventions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../execution_model/index.html">Execution Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_model/index.html">Data model</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">Primitives</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../general.html">Common definitions</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">Attributes</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="scratchpad.html">Scratchpad Mode</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="post-ops.html">Post-ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#attribute-related-error-handling">Attribute Related Error Handling</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#api">API</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../batch_normalization.html">Batch Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../binary.html">Binary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../concat.html">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../convolution.html">Convolution and Deconvolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../eltwise.html">Elementwise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inner-product.html">Inner Product</a></li>
<li class="toctree-l3"><a class="reference internal" href="../layer_normalization.html">Layer normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../logsoftmax.html">LogSoftmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lrn.html">Local Response Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pooling.html">Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reorder.html">Reorder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../resampling.html">Resampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../rnn.html">RNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../shuffle.html">Shuffle</a></li>
<li class="toctree-l3"><a class="reference internal" href="../softmax.html">Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sum.html">Sum</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id1">Open Source Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#implementation-notes">Implementation Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneCCL/source/index.html">oneCCL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../l0/source/index.html">Level Zero</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneDAL/source/index.html">oneDAL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneTBB/source/nested-index.html">oneTBB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneVPL/source/index.html">oneVPL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneMKL/source/index.html">oneMKL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../contributors.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../versions.html">HTML and PDF Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notices.html">Legal Notices and Disclaimers</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">oneAPI Specification</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html">oneAPI Specification</a> &raquo;</li>
        
          
          <li><a href="../../index.html">oneDNN</a> </li>
          
        
          
        
          
        
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/oneapi-src/oneapi-spec/blob/master/source/elements/oneDNN/source/primitives/attributes/quantization.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quantization">
<span id="attributes-quantization-label"></span><h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">Â¶</a></h1>
<p>Primitives may support reduced precision computations which require
quantization.</p>
<div class="section" id="quantization-model">
<h2>Quantization Model<a class="headerlink" href="#quantization-model" title="Permalink to this headline">Â¶</a></h2>
<p>The primary quantization model that the library assumes is the following:</p>
<div class="math notranslate nohighlight">
\[x_{f32}[:] = scale_{f32} \cdot (x_{int8}[:] - 0_{x_{int8}})\]</div>
<p>where <span class="math notranslate nohighlight">\(scale_{f32}\)</span> is a <em>scaling factor</em> that is somehow known in
advance and <span class="math notranslate nohighlight">\([:]\)</span> is used to denote elementwise application of the
formula to the arrays. Typically, the process of computing scale factors is
called <em>calibration</em>. The library cannot compute any of the scale factors at
run-time dynamically.  Hence, the model is sometimes called a <em>static</em>
quantization model. The main rationale to support only <em>static</em> quantization
out-of-the-box is higher performance. To use <em>dynamic</em> quantization:</p>
<ol class="arabic simple">
<li><p>Compute the result in higher precision, like
<a class="reference internal" href="../../data_model/data_types/index.html#_CPPv4N4dnnl6memory9data_type3s32E" title="dnnl::memory::data_type::s32"><code class="xref cpp cpp-enumerator docutils literal notranslate"><span class="pre">dnnl::memory::data_type::s32</span></code></a>.</p></li>
<li><p>Find the required characteristics, like min and max values, and derive the
scale factor.</p></li>
<li><p>Re-quantize to the lower precision data type.</p></li>
</ol>
<p>oneDNN assumes a fixed zero position. For most of the primitives, the real
zero value is mapped to the zero for quantized values; that is,
<span class="math notranslate nohighlight">\(0_{x_{int8}} = 0\)</span>. For example, this is the only model that
<a class="reference internal" href="../convolution.html#convolution-label"><span class="std std-ref">Convolution and Deconvolution</span></a> and <a class="reference internal" href="../inner-product.html#inner-product-label"><span class="std std-ref">Inner Product</span></a> currently support.
The <a class="reference internal" href="../rnn.html#rnn-label"><span class="std std-ref">RNN</span></a> primitives have limited support of shifted zero.</p>
<p>For the rest of this section we that <span class="math notranslate nohighlight">\(0_{x_{int8}} = 0\)</span>.</p>
<div class="section" id="example-convolution-quantization-workflow">
<h3>Example: Convolution Quantization Workflow<a class="headerlink" href="#example-convolution-quantization-workflow" title="Permalink to this headline">Â¶</a></h3>
<p>Consider a convolution without bias. The tensors are represented as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\src_{f32}[:] = scale_{\src} \cdot \src_{int8}[:]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\weights_{f32}[:] = scale_{\weights} \cdot \weights_{int8}[:]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dst_{f32}[:] = scale_{\dst} \cdot \dst_{int8}[:]\)</span></p></li>
</ul>
<p>Here the <span class="math notranslate nohighlight">\(\src_{f32}, \weights_{f32}, \dst_{f32}\)</span> are not computed at
all, the whole work happens with int8 tensors.  As mentioned above, we also
somehow know all the scaling factors: <cite>scale_{src}, scale_{weights},
scale_{dst}</cite>.</p>
<p>So the task is to compute the <span class="math notranslate nohighlight">\(\dst_{int8}\)</span> tensor.</p>
<p>Mathematically, the computations are:</p>
<div class="math notranslate nohighlight">
\[\dst_{int8}[:] =
   \operatorname{f32\_to\_int8}(
      output\_scale \cdot
      conv_{s32}(\src_{int8}, \weights_{int8})
   ),\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(output\_scale := \frac{scale_{\src} \cdot scale_{\weights}}{scale_{\dst}}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(conv_{s32}\)</span> is just a regular convolution which takes source and
weights with int8 data type and compute the result in int32 data type (int32
is chosen to avoid overflows during the computations);</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{f32\_to\_s8}()\)</span> converts an <cite>f32</cite> value to <cite>s8</cite> with
potential saturation if the values are out of the range of the int8 data
type.</p></li>
</ul>
<p>Note that in order to perform the operation, one doesnât need to know the
exact scaling factors for all the tensors; it is enough to know only the
<cite>output_scale</cite>. The library utilizes this fact: a user needs to provide only
this one extra parameter to the convolution primitive (see the
<a class="reference internal" href="#output-scaling-label"><span class="std std-ref">Output Scaling Attribute</span></a> section below).</p>
</div>
<div class="section" id="per-channel-scaling">
<h3>Per-Channel Scaling<a class="headerlink" href="#per-channel-scaling" title="Permalink to this headline">Â¶</a></h3>
<p>Primitives may have limited support of multiple scales for a quantized tensor.
The most popular use case is the <a class="reference internal" href="../convolution.html#convolution-label"><span class="std std-ref">Convolution and Deconvolution</span></a> primitives that
support per-output-channel scaling factors for the weights, meaning that the
actual convolution computations would need to scale different output channels
differently.</p>
<p>Let <span class="math notranslate nohighlight">\(\alpha\)</span> denote scales:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\src_{f32}(n, ic, ih, iw) = \alpha_{\src} \cdot \src_{int8}(n, ic, ih, iw)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\weights_{f32}(oc, ic, kh, kw) = \alpha_{\weights}(oc) \cdot \weights_{int8}(oc, ic, kh, kw)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dst_{f32}(n, oc, oh, ow) = scale_{\dst} \cdot \dst_{int8}(n, oc, oh, ow)\)</span></p></li>
</ul>
<p>Note that now the weightsâ scaling factor depends on the <span class="math notranslate nohighlight">\(oc\)</span>.</p>
<p>To compute the <span class="math notranslate nohighlight">\(\dst_{int8}\)</span> we need to perform the following:</p>
<div class="math notranslate nohighlight">
\[\dst_{int8}(n, oc, oh, ow) =
    \operatorname{f32\_to\_int8}(
        output\_scale(oc) \cdot
        conv_{s32}(\src_{int8}, \weights_{int8})|_{(n, oc, oh, ow)}
    ),\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[output\_scale(oc) :=
 \frac{\alpha_{\src} \cdot \alpha_{\weights}(oc)}{\alpha_{\dst}}.\]</div>
<p>The user is responsible for preparing quantized weights accordingly. To do
that, oneDNN provides reorders that can perform per-channel scaling:</p>
<div class="math notranslate nohighlight">
\[\weights_{int8}(oc, ic, kh, kw) =
    \operatorname{f32\_to\_int8}(
        output\_scale(oc) \cdot
        \weights_{f32}(oc, ic, kh, kw)
    ),\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[output\_scale(oc) := \frac{1}{\alpha_{\weights}(oc_{})}.\]</div>
</div>
</div>
<div class="section" id="output-scaling-attribute">
<span id="output-scaling-label"></span><h2>Output Scaling Attribute<a class="headerlink" href="#output-scaling-attribute" title="Permalink to this headline">Â¶</a></h2>
<p>oneDNN provides <a class="reference internal" href="index.html#_CPPv4N4dnnl14primitive_attr17set_output_scalesEiRKNSt6vectorIfEE" title="dnnl::primitive_attr::set_output_scales"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::primitive_attr::set_output_scales()</span></code></a> for setting
scaling factors for most of the primitives.</p>
<p>The primitives may not support output scales if source (and weights) tensors
are not of the int8 data type. In other words, convolution operating on the
single precision floating point data type may not scale the output result.</p>
<p>In the simplest case, when there is only one common scale the attribute
changes the op behavior from</p>
<div class="math notranslate nohighlight">
\[\dst[:] = Op(...)\]</div>
<p>to</p>
<div class="math notranslate nohighlight">
\[\dst[:] = scale \cdot Op(...).\]</div>
<p>To support scales per one or several dimensions, users must set the appropriate
mask.</p>
<p>Say the primitive destination is a <span class="math notranslate nohighlight">\(D_0 \times ... \times D_{n-1}\)</span>
tensor and we want to have output scales per <span class="math notranslate nohighlight">\(d_i\)</span> dimension (where
<span class="math notranslate nohighlight">\(0 \le d_i &lt; n\)</span>).</p>
<p>Then <span class="math notranslate nohighlight">\(mask = \sum \limits_{d_i} 2^{d_i}\)</span> and the number of scales should be
<span class="math notranslate nohighlight">\(\mathtt{scales.size()} = \prod \limits_{d_i} D_{d_i}\)</span>.</p>
<p>The scaling happens in the single precision floating point data type
(<code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">data_type::f32</span></code>). Before storing, the result is converted to the
destination data type with saturation if required. The rounding happens
according to the current hardware setting.</p>
<div class="section" id="example-1-weights-quantization-with-per-output-channel-and-group-scaling">
<h3>Example 1: weights quantization with per-output-channel-and-group scaling<a class="headerlink" href="#example-1-weights-quantization-with-per-output-channel-and-group-scaling" title="Permalink to this headline">Â¶</a></h3>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// weights dimensions</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">G</span><span class="p">,</span> <span class="n">OC</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">;</span>

<span class="c1">// original f32 weights in plain format</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">wei_plain_f32_md</span><span class="p">(</span>
        <span class="p">{</span><span class="n">G</span><span class="p">,</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">IC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">},</span>          <span class="c1">// dims</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span>     <span class="c1">// the data originally in f32</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">hwigo</span>   <span class="c1">// the plain memory format</span>
        <span class="p">);</span>

<span class="c1">// the scaling factors for quantized weights</span>
<span class="c1">// An unique scale for each group and output-channel.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span> <span class="cm">/* values */</span> <span class="p">};</span>

<span class="c1">// int8 convolution primitive descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">primitive_desc</span> <span class="n">conv_pd</span><span class="p">(</span><span class="cm">/* see the next example */</span><span class="p">);</span>

<span class="c1">// query the convolution weights memory descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">wei_conv_s8_md</span> <span class="o">=</span> <span class="n">conv_pd</span><span class="p">.</span><span class="n">weights_desc</span><span class="p">();</span>

<span class="c1">// prepare the inverse of the scales</span>
<span class="c1">// (f32 = scale * int8 --&gt; int8 = 1/scale * f32)</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">inv_wei_scales</span><span class="p">(</span><span class="n">wei_scales</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">wei_scales</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="n">inv_wei_scales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.f</span> <span class="o">/</span> <span class="n">wei_scales</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

<span class="c1">// prepare the attributes for the reorder</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1">// scale per  G dimension, which is the dim #0</span>
    <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// scale per OC dimension, which is the dim #1</span>
<span class="n">attr</span><span class="p">.</span><span class="n">set_output_scales</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">inv_wei_scales</span><span class="p">);</span>

<span class="c1">// create reorder that would perform:</span>
<span class="c1">//   wei_s8(g, oc, ic, kh, kw) &lt;- 1/scale(g, oc) * wei_f32(g, oc, ic, kh, kw)</span>
<span class="c1">// including the data format transformation.</span>
<span class="k">auto</span> <span class="n">wei_reorder_pd</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">reorder</span><span class="o">::</span><span class="n">primitive_desc</span><span class="p">(</span>
        <span class="n">wei_plain_f32_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="c1">// source</span>
        <span class="n">wei_conv_s8_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="c1">// destination,</span>
        <span class="n">attr</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">wei_reorder</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">reorder</span><span class="p">(</span><span class="n">wei_reorder_pd</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="example-2-convolution-with-groups-with-per-output-channel-quantization">
<h3>Example 2: convolution with groups, with per-output-channel quantization<a class="headerlink" href="#example-2-convolution-with-groups-with-per-output-channel-quantization" title="Permalink to this headline">Â¶</a></h3>
<p>This example is complementary to the previous example (which should ideally be
the first one). Letâs say we want to create an int8 convolution with
per-output channel scaling.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span> <span class="kt">float</span> <span class="n">src_scale</span><span class="p">;</span> <span class="c1">// src_f32[:] = src_scale * src_s8[:]</span>
<span class="k">const</span> <span class="kt">float</span> <span class="n">dst_scale</span><span class="p">;</span> <span class="c1">// dst_f32[:] = dst_scale * dst_s8[:]</span>

<span class="c1">// the scaling factors for quantized weights (as declared above)</span>
<span class="c1">// An unique scale for each group and output-channel.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>


<span class="c1">// Src, weights, and dst memory descriptors for convolution,</span>
<span class="c1">// with memory format tag == any to allow a convolution implementation</span>
<span class="c1">// to chose the appropriate memory format</span>

<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">src_conv_s8_any_md</span><span class="p">(</span>
        <span class="p">{</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">IH</span><span class="p">,</span> <span class="n">IW</span><span class="p">},</span>            <span class="c1">// dims</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">s8</span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">any</span> <span class="c1">// let convolution to choose</span>
        <span class="p">);</span>

<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">wei_conv_s8_any_md</span><span class="p">(</span>
        <span class="p">{</span><span class="n">G</span><span class="p">,</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">IC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">},</span>        <span class="c1">// dims</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">s8</span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">any</span> <span class="c1">// let convolution to choose</span>
        <span class="p">);</span>

<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">dst_conv_s8_any_md</span><span class="p">(...);</span>  <span class="c1">// ditto</span>

<span class="c1">// Create a convolution operation descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">desc</span> <span class="n">conv_d</span><span class="p">(</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">prop_kind</span><span class="o">::</span><span class="n">forward_inference</span><span class="p">,</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">algorithm</span><span class="o">::</span><span class="n">convolution_direct</span><span class="p">,</span>
        <span class="n">src_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// what&#39;s important is that</span>
        <span class="n">wei_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// we specified that we want</span>
        <span class="n">dst_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// computations in s8</span>
        <span class="n">strides</span><span class="p">,</span> <span class="n">padding_l</span><span class="p">,</span> <span class="n">padding_r</span><span class="p">,</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">padding_kind</span><span class="o">::</span><span class="n">zero</span>
        <span class="p">);</span>

<span class="c1">// prepare the attributes for the convolution</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// scale per OC dimension, which is the dim #1 on dst tensor:</span>
                <span class="c1">// (BATCH, OC, OH, OW)</span>
                <span class="c1">//    0     1   2   3</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">conv_output_scales</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">g_oc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">;</span> <span class="o">++</span><span class="n">g_oc</span><span class="p">)</span>
    <span class="n">conv_output_scales</span><span class="p">[</span><span class="n">g_oc</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_scale</span> <span class="o">*</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">g_oc</span><span class="p">)</span> <span class="o">/</span> <span class="n">dst_scale</span><span class="p">;</span>
<span class="n">attr</span><span class="p">.</span><span class="n">set_output_scales</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">conv_output_scales</span><span class="p">);</span>

<span class="c1">// create a convolution primitive descriptor with the scaling factors</span>
<span class="k">auto</span> <span class="n">conv_pd</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">primitive_desc</span><span class="p">(</span>
        <span class="n">conv_d</span><span class="p">,</span> <span class="c1">// general (non-customized) operation descriptor</span>
        <span class="n">attr</span><span class="p">,</span>   <span class="c1">// the attributes contain the output scaling</span>
        <span class="n">engine</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="interplay-of-output-scales-with-post-ops">
<h3>Interplay of Output Scales with Post-ops<a class="headerlink" href="#interplay-of-output-scales-with-post-ops" title="Permalink to this headline">Â¶</a></h3>
<p>In general, the <a class="reference internal" href="post-ops.html#post-ops-label"><span class="std std-ref">Post-ops</span></a> are independent from the output scales.
The output scales are applied to the result first; then post-ops will take
effect.</p>
<p>That has an implication on the scaling factors passed to the library, however.
Consider the following example of a convolution with <span class="math notranslate nohighlight">\(\tanh\)</span> post-op:</p>
<div class="math notranslate nohighlight">
\[\dst_{s8}[:] =
    \frac{1}{scale_{\dst}}
    \cdot
    \tanh(
            scale_{\src}
            \cdot
            scale_{\weights}
            \cdot
            conv_{s32}(\src_{s8}, wei_{s8})
    )\]</div>
<ul class="simple">
<li><p>The convolution output scales are
<span class="math notranslate nohighlight">\(conv\_output\_scale = scale_{\src} \cdot scale_{\weights}\)</span>,
i.e. there is no division by <span class="math notranslate nohighlight">\(scale_{\dst}\)</span>.</p></li>
<li><p>And the post-ops scale for <span class="math notranslate nohighlight">\(\tanh\)</span> is set to
<span class="math notranslate nohighlight">\(scale\_tanh\_post\_op = \frac{1}{scale_{\dst}}\)</span>.</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="post-ops.html" class="btn btn-neutral float-right" title="Post-ops" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="scratchpad.html" class="btn btn-neutral float-left" title="Scratchpad Mode" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Intel Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>