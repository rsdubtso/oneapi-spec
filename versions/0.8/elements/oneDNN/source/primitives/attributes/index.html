

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Attributes &mdash; oneAPI Specification 0.8 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../../../_static/favicons.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/language_data.js"></script>
        <script src="../../../../../_static/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="Post-ops" href="post-ops.html" />
    <link rel="prev" title="Common Definitions" href="../general.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="https://oneapi.com" class="icon icon-home"> oneAPI Specification
          

          
            
            <img src="../../../../../_static/oneAPI-rgb-rev-100.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../architecture.html">Software Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../library-interop.html">Library Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../elements.html">oneAPI Elements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dpcpp/source/index.html">DPC++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneDPL/source/index.html">oneDPL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">oneDNN</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../conventions.html">Conventions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../execution_model/index.html">Execution Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_model/index.html">Data model</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">Primitives</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../general.html">Common Definitions</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Attributes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="post-ops.html">Post-ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scratchpad-mode">Scratchpad Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute-related-error-handling">Attribute Related Error Handling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#api">API</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../batch_normalization.html">Batch Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../binary.html">Binary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../concat.html">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../convolution.html">Convolution and Deconvolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../eltwise.html">Elementwise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inner-product.html">Inner Product</a></li>
<li class="toctree-l3"><a class="reference internal" href="../layer_normalization.html">Layer normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../logsoftmax.html">LogSoftmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lrn.html">Local Response Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pooling.html">Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reorder.html">Reorder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../resampling.html">Resampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../rnn.html">RNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../shuffle.html">Shuffle</a></li>
<li class="toctree-l3"><a class="reference internal" href="../softmax.html">Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sum.html">Sum</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id1">Open Source Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#implementation-notes">Implementation Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneCCL/source/index.html">oneCCL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../l0/source/index.html">Level Zero</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneDAL/source/index.html">oneDAL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneTBB/source/nested-index.html">oneTBB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneVPL/source/index.html">oneVPL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../oneMKL/source/index.html">oneMKL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../contributors.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../versions.html">HTML and PDF Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notices.html">Legal Notices and Disclaimers</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">oneAPI Specification</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html">oneAPI Specification</a> &raquo;</li>
        
          
          <li><a href="../../index.html">oneDNN</a> </li>
          
        
          
        
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/oneapi-src/oneapi-spec/blob/master/source/elements/oneDNN/source/primitives/attributes/index.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="attributes">
<span id="attributes-link"></span><h1>Attributes<a class="headerlink" href="#attributes" title="Permalink to this headline">¶</a></h1>
<p>The parameters passed to create a primitive descriptor specify the problem. An
engine specifies where the primitive will be executed. An operation descriptor
specifies the basics: the operation kind; the propagation kind; the source,
destination, and other tensors; the strides (if applicable); and so on.</p>
<p><em>Attributes</em> specify some extra properties of the primitive. Users must
create them before use and must set required specifics using the corresponding
setters. The attributes are copied during primitive descriptor creation, so
users can change or destroy attributes right after that.</p>
<p>If not modified, attributes can stay empty, which is equivalent to the default
attributes. Primitive descriptors’ constructors have empty attributes as
default parameters, so unless required users can simply omit them.</p>
<p>Attributes can also contain <em>post-ops</em>, which are computations executed after
the primitive.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="post-ops.html">Post-ops</a></li>
</ul>
</div>
<div class="section" id="scratchpad-mode">
<h2>Scratchpad Mode<a class="headerlink" href="#scratchpad-mode" title="Permalink to this headline">¶</a></h2>
<p>Some primitives might require a temporary buffer while performing their
computations. For instance, the operations that do not have enough independent
work to utilize all cores on a system might use parallelization over the
reduction dimension (the K dimension in the GEMM notation). In this case
different threads compute partial results in private temporary buffers, and
then the private results are added to produce the final result. Another
example is using matrix multiplication (GEMM) to implement convolution. Before
calling GEMM, the source activations need to be transformed using the
<code class="docutils literal notranslate"><span class="pre">im2col</span></code> operation. The transformation result is written to a temporary
buffer that is then used as an input for the GEMM.</p>
<p>In both of these examples, the temporary buffer is no longer required once the
primitive computation is completed. oneDNN refers to such kind of a memory
buffer as a <em>scratchpad</em>.</p>
<p>Both types of implementation might need extra space for the reduction in case
there are too few independent tasks. The amount of memory required by the
<code class="docutils literal notranslate"><span class="pre">im2col</span></code> transformation is proportional to the size of the source image
multiplied by the weights spatial size. The size of a buffer for reduction is
proportional to the tensor size to be reduced (e.g., <code class="docutils literal notranslate"><span class="pre">diff_weights</span></code> in the
case of backward by weights) multiplied by the number of threads in the
reduction groups (the upper bound is the total number of threads).</p>
<p>By contrast, some other primitives might require very little extra space. For
instance, one of the implementation of the <a class="reference internal" href="../sum.html#_CPPv4N4dnnl3sumE" title="dnnl::sum"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::sum</span></code></a> primitive requires
temporary space only to store the pointers to data for each and every input
array (that is, the size of the scratchpad is <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">*</span> <span class="pre">sizeof(void</span> <span class="pre">*)</span></code>, where
<code class="docutils literal notranslate"><span class="pre">n</span></code> is the number of summands).</p>
<p>oneDNN supports two modes for handling scratchpads:</p>
<dl class="cpp enum">
<dt id="_CPPv4N4dnnl15scratchpad_modeE">
<span id="_CPPv3N4dnnl15scratchpad_modeE"></span><span id="_CPPv2N4dnnl15scratchpad_modeE"></span><span class="target" id="group__dnnl__api__attributes_1gac24d40ceea0256c7d6cc3a383a0fa07f"></span><em class="property">enum </em><code class="sig-prename descclassname">dnnl<code class="sig-prename descclassname">::</code></code><code class="sig-name descname">scratchpad_mode</code><a class="headerlink" href="#_CPPv4N4dnnl15scratchpad_modeE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Scratchpad mode. </p>
<p><em>Values:</em></p>
<dl class="cpp enumerator">
<dt id="_CPPv4N4dnnl15scratchpad_mode7libraryE">
<span id="_CPPv3N4dnnl15scratchpad_mode7libraryE"></span><span id="_CPPv2N4dnnl15scratchpad_mode7libraryE"></span><span class="target" id="group__dnnl__api__attributes_1ggac24d40ceea0256c7d6cc3a383a0fa07fad521f765a49c72507257a2620612ee96"></span><em class="property">enumerator </em><code class="sig-name descname">library</code><a class="headerlink" href="#_CPPv4N4dnnl15scratchpad_mode7libraryE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The library manages the scratchpad allocation. There may be multiple implementation-specific policies that can be configured via mechanisms that fall outside of the scope of this specification. </p>
</dd></dl>

<dl class="cpp enumerator">
<dt id="_CPPv4N4dnnl15scratchpad_mode4userE">
<span id="_CPPv3N4dnnl15scratchpad_mode4userE"></span><span id="_CPPv2N4dnnl15scratchpad_mode4userE"></span><span class="target" id="group__dnnl__api__attributes_1ggac24d40ceea0256c7d6cc3a383a0fa07faee11cbb19052e40b07aac0ca060c23ee"></span><em class="property">enumerator </em><code class="sig-name descname">user</code><a class="headerlink" href="#_CPPv4N4dnnl15scratchpad_mode4userE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The user manages the scratchpad allocation by querying and providing the scratchpad memory to primitives. This mode is thread-safe as long as the scratchpad buffers are not used concurrently by two primitive executions. </p>
</dd></dl>

</dd></dl>

<p>The scratchpad mode is controlled though the
<a class="reference internal" href="#_CPPv4N4dnnl14primitive_attr19set_scratchpad_modeE15scratchpad_mode" title="dnnl::primitive_attr::set_scratchpad_mode"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::primitive_attr::set_scratchpad_mode()</span></code></a> primitive attributes.</p>
<p>If the user provides scratchpad memory to a primitive, this memory must be
created using the same engine that the primitive uses.</p>
<p>All primitives support both scratchpad modes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Primitives are not thread-safe by default. The only way to make the
primitive execution fully thread-safe is to use the
<a class="reference internal" href="#_CPPv4N4dnnl15scratchpad_mode4userE" title="dnnl::scratchpad_mode::user"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::scratchpad_mode::user</span></code></a> mode and not pass the same scratchpad
memory to two primitives that are executed concurrently.</p>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<div class="section" id="library-manages-scratchpad">
<h4>Library Manages Scratchpad<a class="headerlink" href="#library-manages-scratchpad" title="Permalink to this headline">¶</a></h4>
<p>As mentioned above, this is a default behavior. We only want to highlight how
a user can query the amount of memory consumed by a primitive due to a
scratchpad.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Use default attr, hence the library allocates scratchpad</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive</span><span class="o">::</span><span class="n">primitive_desc</span> <span class="n">op_pd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="cm">/* other arguments */</span><span class="p">);</span>

<span class="c1">// Print how much memory would be hold by a primitive due to scratchpad</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;primitive will use &quot;</span>
          <span class="o">&lt;&lt;</span> <span class="n">op_pd</span><span class="p">.</span><span class="n">query_s64</span><span class="p">(</span><span class="n">dnnl</span><span class="o">::</span><span class="n">query</span><span class="o">::</span><span class="n">memory_consumption_s64</span><span class="p">)</span>
          <span class="o">&lt;&lt;</span> <span class="s">&quot; bytes&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// In this case scratchpad is internal, hence user visible scratchpad memory</span>
<span class="c1">// descriptor should be empty:</span>
<span class="k">auto</span> <span class="n">zero_md</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="section" id="user-manages-scratchpad">
<h4>User Manages Scratchpad<a class="headerlink" href="#user-manages-scratchpad" title="Permalink to this headline">¶</a></h4>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create an empty (default) attributes</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>

<span class="c1">// Default scratchpad mode is `library`:</span>
<span class="n">assert</span><span class="p">(</span><span class="n">attr</span><span class="p">.</span><span class="n">get_scratchpad_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">scratchpad_mode</span><span class="o">::</span><span class="n">library</span><span class="p">);</span>

<span class="c1">// Set scratchpad mode to `user`</span>
<span class="n">attr</span><span class="p">.</span><span class="n">set_scratchpad_mode</span><span class="p">(</span><span class="n">dnnl</span><span class="o">::</span><span class="n">scratchpad_mode</span><span class="o">::</span><span class="n">user</span><span class="p">);</span>

<span class="c1">// Create a primitive descriptor with custom attributes</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive</span><span class="o">::</span><span class="n">primitive_desc</span> <span class="n">op_pd</span><span class="p">(</span><span class="n">op_d</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">engine</span><span class="p">);</span>

<span class="c1">// Query the scratchpad memory descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">scratchpad_md</span> <span class="o">=</span> <span class="n">op_pd</span><span class="p">.</span><span class="n">scratchpad_desc</span><span class="p">();</span>

<span class="c1">// Note, that a primitive doesn&#39;t consume memory in this configuration:</span>
<span class="n">assert</span><span class="p">(</span><span class="n">op_pd</span><span class="p">.</span><span class="n">query_s64</span><span class="p">(</span><span class="n">dnnl</span><span class="o">::</span><span class="n">query</span><span class="o">::</span><span class="n">memory_consumption_s64</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">// Create a primitive</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive</span> <span class="n">prim</span><span class="p">(</span><span class="n">op_pd</span><span class="p">);</span>

<span class="c1">// ... more code ..</span>

<span class="c1">// Create a scratchpad memory</span>
<span class="c1">// NOTE: if scratchpad is not required for a particular primitive the</span>
<span class="c1">//       scratchpad_md.get_size() will return 0. It is fine to have</span>
<span class="c1">//       scratchpad_ptr == nullptr in this case.</span>
<span class="kt">void</span> <span class="o">*</span><span class="n">scratchpad_ptr</span> <span class="o">=</span> <span class="n">user_memory_manager</span><span class="o">::</span><span class="n">allocate</span><span class="p">(</span><span class="n">scratchpad_md</span><span class="p">.</span><span class="n">get_size</span><span class="p">());</span>
<span class="c1">// NOTE: engine here must much the engine of the primitive</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span> <span class="n">scratchpad</span><span class="p">(</span><span class="n">scratchpad_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="n">scratchpad_ptr</span><span class="p">);</span>

<span class="c1">// Pass a scratchpad memory to a primitive</span>
<span class="n">prim</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="p">{</span> <span class="cm">/* other arguments */</span><span class="p">,</span>
        <span class="p">{</span><span class="n">DNNL_ARG_SCRATCHPAD</span><span class="p">,</span> <span class="n">scratchpad</span><span class="p">}});</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="quantization">
<span id="attributes-quantization-label"></span><h2>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">¶</a></h2>
<p>Primitives may support reduced precision computations which require
quantization.</p>
<div class="section" id="quantization-model">
<h3>Quantization Model<a class="headerlink" href="#quantization-model" title="Permalink to this headline">¶</a></h3>
<p>The primary quantization model that the library assumes is the following:</p>
<div class="math notranslate nohighlight">
\[x_{f32}[:] = scale_{f32} \cdot (x_{int8}[:] - 0_{x_{int8}})\]</div>
<p>where <span class="math notranslate nohighlight">\(scale_{f32}\)</span> is a <em>scaling factor</em> that is somehow known in
advance and <span class="math notranslate nohighlight">\([:]\)</span> is used to denote elementwise application of the
formula to the arrays. Typically, the process of computing scale factors is
called <em>calibration</em>. The library cannot compute any of the scale factors at
run-time dynamically.  Hence, the model is sometimes called a <em>static</em>
quantization model. The main rationale to support only <em>static</em> quantization
out-of-the-box is higher performance. To use <em>dynamic</em> quantization:</p>
<ol class="arabic simple">
<li><p>Compute the result in higher precision, like
<a class="reference internal" href="../../data_model/data_types/index.html#_CPPv4N4dnnl6memory9data_type3s32E" title="dnnl::memory::data_type::s32"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::memory::data_type::s32</span></code></a>.</p></li>
<li><p>Find the required characteristics, like min and max values, and derive the
scale factor.</p></li>
<li><p>Re-quantize to the lower precision data type.</p></li>
</ol>
<p>oneDNN assumes a fixed zero position. For most of the primitives, the real
zero value is mapped to the zero for quantized values; that is,
<span class="math notranslate nohighlight">\(0_{x_{int8}} = 0\)</span>. For example, this is the only model that
<a class="reference internal" href="../convolution.html#convolution-label"><span class="std std-ref">Convolution and Deconvolution</span></a> and <a class="reference internal" href="../inner-product.html#inner-product-label"><span class="std std-ref">Inner Product</span></a> currently support.
The <a class="reference internal" href="../rnn.html#rnn-label"><span class="std std-ref">RNN</span></a> primitives have limited support of shifted zero.</p>
<p>For the rest of this section we that <span class="math notranslate nohighlight">\(0_{x_{int8}} = 0\)</span>.</p>
<div class="section" id="example-convolution-quantization-workflow">
<h4>Example: Convolution Quantization Workflow<a class="headerlink" href="#example-convolution-quantization-workflow" title="Permalink to this headline">¶</a></h4>
<p>Consider a convolution without bias. The tensors are represented as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\src_{f32}[:] = scale_{\src} \cdot \src_{int8}[:]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\weights_{f32}[:] = scale_{\weights} \cdot \weights_{int8}[:]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dst_{f32}[:] = scale_{\dst} \cdot \dst_{int8}[:]\)</span></p></li>
</ul>
<p>Here the <span class="math notranslate nohighlight">\(\src_{f32}, \weights_{f32}, \dst_{f32}\)</span> are not computed at
all, the whole work happens with int8 tensors.  As mentioned above, we also
somehow know all the scaling factors: <cite>scale_{src}, scale_{weights},
scale_{dst}</cite>.</p>
<p>So the task is to compute the <span class="math notranslate nohighlight">\(\dst_{int8}\)</span> tensor.</p>
<p>Mathematically, the computations are:</p>
<div class="math notranslate nohighlight">
\[\dst_{int8}[:] =
   \operatorname{f32\_to\_int8}(
      output\_scale \cdot
      conv_{s32}(\src_{int8}, \weights_{int8})
   ),\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(output\_scale := \frac{scale_{\src} \cdot scale_{\weights}}{scale_{\dst}}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(conv_{s32}\)</span> is just a regular convolution which takes source and
weights with int8 data type and compute the result in int32 data type (int32
is chosen to avoid overflows during the computations);</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{f32\_to\_s8}()\)</span> converts an <cite>f32</cite> value to <cite>s8</cite> with
potential saturation if the values are out of the range of the int8 data
type.</p></li>
</ul>
<p>Note that in order to perform the operation, one doesn’t need to know the
exact scaling factors for all the tensors; it is enough to know only the
<cite>output_scale</cite>. The library utilizes this fact: a user needs to provide only
this one extra parameter to the convolution primitive (see the
<a class="reference internal" href="#output-scaling-label"><span class="std std-ref">Output Scaling Attribute</span></a> section below).</p>
</div>
<div class="section" id="per-channel-scaling">
<h4>Per-Channel Scaling<a class="headerlink" href="#per-channel-scaling" title="Permalink to this headline">¶</a></h4>
<p>Primitives may have limited support of multiple scales for a quantized tensor.
The most popular use case is the <a class="reference internal" href="../convolution.html#convolution-label"><span class="std std-ref">Convolution and Deconvolution</span></a> primitives that
support per-output-channel scaling factors for the weights, meaning that the
actual convolution computations would need to scale different output channels
differently.</p>
<p>Let <span class="math notranslate nohighlight">\(\alpha\)</span> denote scales:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\src_{f32}(n, ic, ih, iw) = \alpha_{\src} \cdot \src_{int8}(n, ic, ih, iw)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\weights_{f32}(oc, ic, kh, kw) = \alpha_{\weights}(oc) \cdot \weights_{int8}(oc, ic, kh, kw)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dst_{f32}(n, oc, oh, ow) = scale_{\dst} \cdot \dst_{int8}(n, oc, oh, ow)\)</span></p></li>
</ul>
<p>Note that now the weights’ scaling factor depends on the <span class="math notranslate nohighlight">\(oc\)</span>.</p>
<p>To compute the <span class="math notranslate nohighlight">\(\dst_{int8}\)</span> we need to perform the following:</p>
<div class="math notranslate nohighlight">
\[\dst_{int8}(n, oc, oh, ow) =
    \operatorname{f32\_to\_int8}(
        output\_scale(oc) \cdot
        conv_{s32}(\src_{int8}, \weights_{int8})|_{(n, oc, oh, ow)}
    ),\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[output\_scale(oc) :=
 \frac{\alpha_{\src} \cdot \alpha_{\weights}(oc)}{\alpha_{\dst}}.\]</div>
<p>The user is responsible for preparing quantized weights accordingly. To do
that, oneDNN provides reorders that can perform per-channel scaling:</p>
<div class="math notranslate nohighlight">
\[\weights_{int8}(oc, ic, kh, kw) =
    \operatorname{f32\_to\_int8}(
        output\_scale(oc) \cdot
        \weights_{f32}(oc, ic, kh, kw)
    ),\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[output\_scale(oc) := \frac{1}{\alpha_{\weights}(oc_{})}.\]</div>
</div>
</div>
<div class="section" id="output-scaling-attribute">
<span id="output-scaling-label"></span><h3>Output Scaling Attribute<a class="headerlink" href="#output-scaling-attribute" title="Permalink to this headline">¶</a></h3>
<p>oneDNN provides <a class="reference internal" href="#_CPPv4N4dnnl14primitive_attr17set_output_scalesEiRKNSt6vectorIfEE" title="dnnl::primitive_attr::set_output_scales"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::primitive_attr::set_output_scales()</span></code></a> for setting
scaling factors for most of the primitives.</p>
<p>The primitives may not support output scales if source (and weights) tensors
are not of the int8 data type. In other words, convolution operating on the
single precision floating point data type may not scale the output result.</p>
<p>In the simplest case, when there is only one common scale the attribute
changes the op behavior from</p>
<div class="math notranslate nohighlight">
\[\dst[:] = Op(...)\]</div>
<p>to</p>
<div class="math notranslate nohighlight">
\[\dst[:] = scale \cdot Op(...).\]</div>
<p>To support scales per one or several dimensions, users must set the appropriate
mask.</p>
<p>Say the primitive destination is a <span class="math notranslate nohighlight">\(D_0 \times ... \times D_{n-1}\)</span>
tensor and we want to have output scales per <span class="math notranslate nohighlight">\(d_i\)</span> dimension (where
<span class="math notranslate nohighlight">\(0 \le d_i &lt; n\)</span>).</p>
<p>Then <span class="math notranslate nohighlight">\(mask = \sum \limits_{d_i} 2^{d_i}\)</span> and the number of scales should be
<span class="math notranslate nohighlight">\(\mathtt{scales.size()} = \prod \limits_{d_i} D_{d_i}\)</span>.</p>
<p>The scaling happens in the single precision floating point data type
(<code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">data_type::f32</span></code>). Before storing, the result is converted to the
destination data type with saturation if required. The rounding happens
according to the current hardware setting.</p>
<div class="section" id="example-1-weights-quantization-with-per-output-channel-and-group-scaling">
<h4>Example 1: weights quantization with per-output-channel-and-group scaling<a class="headerlink" href="#example-1-weights-quantization-with-per-output-channel-and-group-scaling" title="Permalink to this headline">¶</a></h4>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// weights dimensions</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">G</span><span class="p">,</span> <span class="n">OC</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">;</span>

<span class="c1">// original f32 weights in plain format</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">wei_plain_f32_md</span><span class="p">(</span>
        <span class="p">{</span><span class="n">G</span><span class="p">,</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">IC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">},</span>          <span class="c1">// dims</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span>     <span class="c1">// the data originally in f32</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">hwigo</span>   <span class="c1">// the plain memory format</span>
        <span class="p">);</span>

<span class="c1">// the scaling factors for quantized weights</span>
<span class="c1">// An unique scale for each group and output-channel.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span> <span class="cm">/* values */</span> <span class="p">};</span>

<span class="c1">// int8 convolution primitive descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">primitive_desc</span> <span class="n">conv_pd</span><span class="p">(</span><span class="cm">/* see the next example */</span><span class="p">);</span>

<span class="c1">// query the convolution weights memory descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">wei_conv_s8_md</span> <span class="o">=</span> <span class="n">conv_pd</span><span class="p">.</span><span class="n">weights_desc</span><span class="p">();</span>

<span class="c1">// prepare the inverse of the scales</span>
<span class="c1">// (f32 = scale * int8 --&gt; int8 = 1/scale * f32)</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">inv_wei_scales</span><span class="p">(</span><span class="n">wei_scales</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">wei_scales</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="n">inv_wei_scales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.f</span> <span class="o">/</span> <span class="n">wei_scales</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

<span class="c1">// prepare the attributes for the reorder</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1">// scale per  G dimension, which is the dim #0</span>
    <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// scale per OC dimension, which is the dim #1</span>
<span class="n">attr</span><span class="p">.</span><span class="n">set_output_scales</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">inv_wei_scales</span><span class="p">);</span>

<span class="c1">// create reorder that would perform:</span>
<span class="c1">//   wei_s8(g, oc, ic, kh, kw) &lt;- 1/scale(g, oc) * wei_f32(g, oc, ic, kh, kw)</span>
<span class="c1">// including the data format transformation.</span>
<span class="k">auto</span> <span class="n">wei_reorder_pd</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">reorder</span><span class="o">::</span><span class="n">primitive_desc</span><span class="p">(</span>
        <span class="n">wei_plain_f32_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="c1">// source</span>
        <span class="n">wei_conv_s8_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="c1">// destination,</span>
        <span class="n">attr</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">wei_reorder</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">reorder</span><span class="p">(</span><span class="n">wei_reorder_pd</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="example-2-convolution-with-groups-with-per-output-channel-quantization">
<h4>Example 2: convolution with groups, with per-output-channel quantization<a class="headerlink" href="#example-2-convolution-with-groups-with-per-output-channel-quantization" title="Permalink to this headline">¶</a></h4>
<p>This example is complementary to the previous example (which should ideally be
the first one). Let’s say we want to create an int8 convolution with
per-output channel scaling.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span> <span class="kt">float</span> <span class="n">src_scale</span><span class="p">;</span> <span class="c1">// src_f32[:] = src_scale * src_s8[:]</span>
<span class="k">const</span> <span class="kt">float</span> <span class="n">dst_scale</span><span class="p">;</span> <span class="c1">// dst_f32[:] = dst_scale * dst_s8[:]</span>

<span class="c1">// the scaling factors for quantized weights (as declared above)</span>
<span class="c1">// An unique scale for each group and output-channel.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>


<span class="c1">// Src, weights, and dst memory descriptors for convolution,</span>
<span class="c1">// with memory format tag == any to allow a convolution implementation</span>
<span class="c1">// to chose the appropriate memory format</span>

<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">src_conv_s8_any_md</span><span class="p">(</span>
        <span class="p">{</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">IH</span><span class="p">,</span> <span class="n">IW</span><span class="p">},</span>            <span class="c1">// dims</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">s8</span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">any</span> <span class="c1">// let convolution to choose</span>
        <span class="p">);</span>

<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">wei_conv_s8_any_md</span><span class="p">(</span>
        <span class="p">{</span><span class="n">G</span><span class="p">,</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">IC</span><span class="o">/</span><span class="n">G</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">},</span>        <span class="c1">// dims</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">s8</span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">any</span> <span class="c1">// let convolution to choose</span>
        <span class="p">);</span>

<span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span> <span class="n">dst_conv_s8_any_md</span><span class="p">(...);</span>  <span class="c1">// ditto</span>

<span class="c1">// Create a convolution operation descriptor</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">desc</span> <span class="n">conv_d</span><span class="p">(</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">prop_kind</span><span class="o">::</span><span class="n">forward_inference</span><span class="p">,</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">algorithm</span><span class="o">::</span><span class="n">convolution_direct</span><span class="p">,</span>
        <span class="n">src_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// what&#39;s important is that</span>
        <span class="n">wei_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// we specified that we want</span>
        <span class="n">dst_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// computations in s8</span>
        <span class="n">strides</span><span class="p">,</span> <span class="n">padding_l</span><span class="p">,</span> <span class="n">padding_r</span><span class="p">,</span>
        <span class="n">dnnl</span><span class="o">::</span><span class="n">padding_kind</span><span class="o">::</span><span class="n">zero</span>
        <span class="p">);</span>

<span class="c1">// prepare the attributes for the convolution</span>
<span class="n">dnnl</span><span class="o">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// scale per OC dimension, which is the dim #1 on dst tensor:</span>
                <span class="c1">// (BATCH, OC, OH, OW)</span>
                <span class="c1">//    0     1   2   3</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">conv_output_scales</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">g_oc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">G</span> <span class="o">*</span> <span class="n">OC</span><span class="o">/</span><span class="n">G</span><span class="p">;</span> <span class="o">++</span><span class="n">g_oc</span><span class="p">)</span>
    <span class="n">conv_output_scales</span><span class="p">[</span><span class="n">g_oc</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_scale</span> <span class="o">*</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">g_oc</span><span class="p">)</span> <span class="o">/</span> <span class="n">dst_scale</span><span class="p">;</span>
<span class="n">attr</span><span class="p">.</span><span class="n">set_output_scales</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">conv_output_scales</span><span class="p">);</span>

<span class="c1">// create a convolution primitive descriptor with the scaling factors</span>
<span class="k">auto</span> <span class="n">conv_pd</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">primitive_desc</span><span class="p">(</span>
        <span class="n">conv_d</span><span class="p">,</span> <span class="c1">// general (non-customized) operation descriptor</span>
        <span class="n">attr</span><span class="p">,</span>   <span class="c1">// the attributes contain the output scaling</span>
        <span class="n">engine</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="interplay-of-output-scales-with-post-ops">
<h4>Interplay of Output Scales with Post-ops<a class="headerlink" href="#interplay-of-output-scales-with-post-ops" title="Permalink to this headline">¶</a></h4>
<p>In general, the <a class="reference internal" href="post-ops.html#post-ops-label"><span class="std std-ref">Post-ops</span></a> are independent from the output scales.
The output scales are applied to the result first; then post-ops will take
effect.</p>
<p>That has an implication on the scaling factors passed to the library, however.
Consider the following example of a convolution with <span class="math notranslate nohighlight">\(\tanh\)</span> post-op:</p>
<div class="math notranslate nohighlight">
\[\dst_{s8}[:] =
    \frac{1}{scale_{\dst}}
    \cdot
    \tanh(
            scale_{\src}
            \cdot
            scale_{\weights}
            \cdot
            conv_{s32}(\src_{s8}, wei_{s8})
    )\]</div>
<ul class="simple">
<li><p>The convolution output scales are
<span class="math notranslate nohighlight">\(conv\_output\_scale = scale_{\src} \cdot scale_{\weights}\)</span>,
i.e. there is no division by <span class="math notranslate nohighlight">\(scale_{\dst}\)</span>.</p></li>
<li><p>And the post-ops scale for <span class="math notranslate nohighlight">\(\tanh\)</span> is set to
<span class="math notranslate nohighlight">\(scale\_tanh\_post\_op = \frac{1}{scale_{\dst}}\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="attribute-related-error-handling">
<span id="attributes-error-handling-link"></span><h2>Attribute Related Error Handling<a class="headerlink" href="#attribute-related-error-handling" title="Permalink to this headline">¶</a></h2>
<p>Since the attributes are created separately from the corresponding primitive
descriptor, consistency checks are delayed.  Users can successfully set
attributes in whatever configuration they want.  However, when they try to
create a primitive descriptor with the attributes they set, it might happen
that there is no primitive implementation that supports such a configuration.
In this case the library will throw the <a class="reference internal" href="../../introduction.html#_CPPv4N4dnnl5errorE" title="dnnl::error"><code class="xref cpp cpp-any docutils literal notranslate"><span class="pre">dnnl::error</span></code></a> exception.</p>
</div>
<div class="section" id="api">
<h2>API<a class="headerlink" href="#api" title="Permalink to this headline">¶</a></h2>
<dl class="cpp struct">
<dt id="_CPPv4N4dnnl14primitive_attrE">
<span id="_CPPv3N4dnnl14primitive_attrE"></span><span id="_CPPv2N4dnnl14primitive_attrE"></span><span id="dnnl::primitive_attr"></span><span class="target" id="structdnnl_1_1primitive__attr"></span><em class="property">struct </em><code class="sig-prename descclassname">dnnl<code class="sig-prename descclassname">::</code></code><code class="sig-name descname">primitive_attr</code><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attrE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Primitive attributes. </p>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric">Public Functions</p>
<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr14primitive_attrEv">
<span id="_CPPv3N4dnnl14primitive_attr14primitive_attrEv"></span><span id="_CPPv2N4dnnl14primitive_attr14primitive_attrEv"></span><span id="dnnl::primitive_attr::primitive_attr"></span><span class="target" id="structdnnl_1_1primitive__attr_1acfbfd85b7ca82bf97e2b07c2427427de"></span><code class="sig-name descname">primitive_attr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr14primitive_attrEv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Constructs default (empty) primitive attributes. </p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4NK4dnnl14primitive_attr19get_scratchpad_modeEv">
<span id="_CPPv3NK4dnnl14primitive_attr19get_scratchpad_modeEv"></span><span id="_CPPv2NK4dnnl14primitive_attr19get_scratchpad_modeEv"></span><span id="dnnl::primitive_attr::get_scratchpad_modeC"></span><span class="target" id="structdnnl_1_1primitive__attr_1af4131b946ec3af3bc2974b603d30029b"></span><a class="reference internal" href="#_CPPv4N4dnnl15scratchpad_modeE" title="dnnl::scratchpad_mode">scratchpad_mode</a> <code class="sig-name descname">get_scratchpad_mode</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em><a class="headerlink" href="#_CPPv4NK4dnnl14primitive_attr19get_scratchpad_modeEv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Returns the scratchpad mode. </p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr19set_scratchpad_modeE15scratchpad_mode">
<span id="_CPPv3N4dnnl14primitive_attr19set_scratchpad_modeE15scratchpad_mode"></span><span id="_CPPv2N4dnnl14primitive_attr19set_scratchpad_modeE15scratchpad_mode"></span><span id="dnnl::primitive_attr::set_scratchpad_mode__scratchpad_mode"></span><span class="target" id="structdnnl_1_1primitive__attr_1a91a597649afa13b7d2416b708d0620d2"></span>void <code class="sig-name descname">set_scratchpad_mode</code><span class="sig-paren">(</span><a class="reference internal" href="#_CPPv4N4dnnl15scratchpad_modeE" title="dnnl::scratchpad_mode">scratchpad_mode</a> <em>mode</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr19set_scratchpad_modeE15scratchpad_mode" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets scratchpad mode.</p>
<p><dl class="simple">
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span></code>: Specified scratchpad mode. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4NK4dnnl14primitive_attr17get_output_scalesERiRNSt6vectorIfEE">
<span id="_CPPv3NK4dnnl14primitive_attr17get_output_scalesERiRNSt6vectorIfEE"></span><span id="_CPPv2NK4dnnl14primitive_attr17get_output_scalesERiRNSt6vectorIfEE"></span><span id="dnnl::primitive_attr::get_output_scales__iR.std::vector:float:RC"></span><span class="target" id="structdnnl_1_1primitive__attr_1aadcfc1f7a787a0b75d6b0daab8da14af"></span>void <code class="sig-name descname">get_output_scales</code><span class="sig-paren">(</span>int &amp;<em>mask</em>, std::vector&lt;float&gt; &amp;<em>scales</em><span class="sig-paren">)</span> <em class="property">const</em><a class="headerlink" href="#_CPPv4NK4dnnl14primitive_attr17get_output_scalesERiRNSt6vectorIfEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Returns output scaling factors correspondence mask and values.</p>
<p><dl class="simple">
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Scaling factors correspondence mask that defines the correspondence between the output tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">scales</span></code> vector. The set i-th bit indicates that a dedicated output scaling factor is used for each index along that dimension. The mask value of 0 implies a common output scaling factor for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scales</span></code>: Vector of output scaling factors. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr17set_output_scalesEiRKNSt6vectorIfEE">
<span id="_CPPv3N4dnnl14primitive_attr17set_output_scalesEiRKNSt6vectorIfEE"></span><span id="_CPPv2N4dnnl14primitive_attr17set_output_scalesEiRKNSt6vectorIfEE"></span><span id="dnnl::primitive_attr::set_output_scales__i.std::vector:float:CR"></span><span class="target" id="structdnnl_1_1primitive__attr_1a4b81acc8e48886313154f75c1708ae02"></span>void <code class="sig-name descname">set_output_scales</code><span class="sig-paren">(</span>int <em>mask</em>, <em class="property">const</em> std::vector&lt;float&gt; &amp;<em>scales</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr17set_output_scalesEiRKNSt6vectorIfEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets output scaling factors correspondence mask and values.</p>
<p>Example usage: <div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">int</span> <span class="n">mb</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">oc</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">oh</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span> <span class="n">ow</span> <span class="o">=</span> <span class="mi">14</span><span class="p">;</span> <span class="o">//</span> <span class="n">convolution</span> <span class="n">output</span> <span class="n">params</span>
<span class="o">//</span> <span class="n">unique</span> <span class="n">output</span> <span class="n">scales</span> <span class="n">per</span> <span class="n">output</span> <span class="n">channel</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span> <span class="n">scales</span> <span class="o">=</span> <span class="p">{</span> <span class="o">...</span> <span class="p">};</span>
<span class="nb">int</span> <span class="n">oc_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="o">//</span> <span class="n">mb_dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">channel_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">height_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="o">...</span>

<span class="o">//</span> <span class="n">construct</span> <span class="n">a</span> <span class="n">convolution</span> <span class="n">descriptor</span>
<span class="n">dnnl</span><span class="p">::</span><span class="n">convolution</span><span class="p">::</span><span class="n">desc</span> <span class="n">conv_d</span><span class="p">;</span>

<span class="n">dnnl</span><span class="p">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>
<span class="n">attr</span><span class="o">.</span><span class="n">set_output_scales</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">oc</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">oc_dim</span><span class="p">,</span> <span class="n">scales</span><span class="p">);</span>

<span class="n">dnnl</span><span class="p">::</span><span class="n">primitive_desc</span> <span class="n">conv_pd</span><span class="p">(</span><span class="n">conv_d</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">engine</span><span class="p">);</span>
</pre></div>
</div>
</p>
<p><dl class="simple">
<dt><strong>Note</strong></dt><dd><p>The order of dimensions does not depend on how elements are laid out in memory. For example:<ul class="simple">
<li><p>for a 2D CNN activations tensor the order is always (n, c)</p></li>
<li><p>for a 4D CNN activations tensor the order is always (n, c, h, w)</p></li>
<li><p>for a 5D CNN weights tensor the order is always (g, oc, ic, kh, kw)</p></li>
</ul>
</p>
</dd>
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Defines the correspondence between the output tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">scales</span></code> vector. The set i-th bit indicates that a dedicated scaling factor is used for each index along that dimension. Set the mask to 0 to use a common output scaling factor for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scales</span></code>: Constant vector of output scaling factors. If the scaling factors are known at the time of this call, the following equality must hold: <span class="math notranslate nohighlight">\(scales.size() = \prod\limits_{d \in mask} output.dims[d].\)</span> Violations can only be detected when the attributes are used to create a primitive descriptor. If the scaling factors are not known at the time of the call, this vector must contain a single <a class="reference internal" href="../general.html#dnnl__types_8h_1ab16365c11b4dc88fbb453edb51f1979f"><span class="std std-ref">DNNL_RUNTIME_F32_VAL</span></a> value and the output scaling factors must be passed at execution time as an argument with index <a class="reference internal" href="../general.html#dnnl__types_8h_1a0afb48b0c2b8f3ee30609aaa47aa29db"><span class="std std-ref">DNNL_ARG_ATTR_OUTPUT_SCALES</span></a>. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4NK4dnnl14primitive_attr10get_scalesEiRiRNSt6vectorIfEE">
<span id="_CPPv3NK4dnnl14primitive_attr10get_scalesEiRiRNSt6vectorIfEE"></span><span id="_CPPv2NK4dnnl14primitive_attr10get_scalesEiRiRNSt6vectorIfEE"></span><span id="dnnl::primitive_attr::get_scales__i.iR.std::vector:float:RC"></span><span class="target" id="structdnnl_1_1primitive__attr_1a9fc7eb58cc958963a2d26743cfd5c763"></span>void <code class="sig-name descname">get_scales</code><span class="sig-paren">(</span>int <em>arg</em>, int &amp;<em>mask</em>, std::vector&lt;float&gt; &amp;<em>scales</em><span class="sig-paren">)</span> <em class="property">const</em><a class="headerlink" href="#_CPPv4NK4dnnl14primitive_attr10get_scalesEiRiRNSt6vectorIfEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Returns scaling factors correspondence mask and values for a given memory argument.</p>
<p><dl class="simple">
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">arg</span></code>: Parameter argument index as passed to the <a class="reference internal" href="../general.html#structdnnl_1_1primitive_1a2c112f2449a18a87310dee2ecd8c64eb"><span class="std std-ref">primitive::execute()</span></a> call. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Scaling factors correspondence mask that defines the correspondence between the output tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">scales</span></code> vector. The set i-th bit indicates that a dedicated scaling factor is used for each index along that dimension. Set the mask to 0 to use a common scaling factor for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scales</span></code>: Output vector of scaling factors. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr10set_scalesEiiRKNSt6vectorIfEE">
<span id="_CPPv3N4dnnl14primitive_attr10set_scalesEiiRKNSt6vectorIfEE"></span><span id="_CPPv2N4dnnl14primitive_attr10set_scalesEiiRKNSt6vectorIfEE"></span><span id="dnnl::primitive_attr::set_scales__i.i.std::vector:float:CR"></span><span class="target" id="structdnnl_1_1primitive__attr_1a9bf717bd25b6fddd89055da8178eab75"></span>void <code class="sig-name descname">set_scales</code><span class="sig-paren">(</span>int <em>arg</em>, int <em>mask</em>, <em class="property">const</em> std::vector&lt;float&gt; &amp;<em>scales</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr10set_scalesEiiRKNSt6vectorIfEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets scaling factors for primitive operations for a given memory argument.</p>
<p><dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference internal" href="#structdnnl_1_1primitive__attr_1a4b81acc8e48886313154f75c1708ae02"><span class="std std-ref">dnnl::primitive_attr::set_output_scales</span></a></p>
</dd>
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">arg</span></code>: Parameter argument index as passed to the <a class="reference internal" href="../general.html#structdnnl_1_1primitive_1a2c112f2449a18a87310dee2ecd8c64eb"><span class="std std-ref">primitive::execute()</span></a> call. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Scaling factors correspondence mask that defines the correspondence between the tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">scales</span></code> vector. The set i-th bit indicates that a dedicated scaling factor is used for each index along that dimension. Set the mask to 0 to use a common scaling factor for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scales</span></code>: Constant vector of scaling factors. The following equality must hold: <span class="math notranslate nohighlight">\(scales.size() = \prod\limits_{d \in mask} argument.dims[d].\)</span> </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4NK4dnnl14primitive_attr15get_zero_pointsEiRiRNSt6vectorI7int32_tEE">
<span id="_CPPv3NK4dnnl14primitive_attr15get_zero_pointsEiRiRNSt6vectorI7int32_tEE"></span><span id="_CPPv2NK4dnnl14primitive_attr15get_zero_pointsEiRiRNSt6vectorI7int32_tEE"></span><span id="dnnl::primitive_attr::get_zero_points__i.iR.std::vector:int32_t:RC"></span><span class="target" id="structdnnl_1_1primitive__attr_1a010dfd54aa839ca1c0da892b99963bdf"></span>void <code class="sig-name descname">get_zero_points</code><span class="sig-paren">(</span>int <em>arg</em>, int &amp;<em>mask</em>, std::vector&lt;int32_t&gt; &amp;<em>zero_points</em><span class="sig-paren">)</span> <em class="property">const</em><a class="headerlink" href="#_CPPv4NK4dnnl14primitive_attr15get_zero_pointsEiRiRNSt6vectorI7int32_tEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Returns zero points correspondence mask and values.</p>
<p><dl class="simple">
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">arg</span></code>: Parameter argument index as passed to the <a class="reference internal" href="../general.html#structdnnl_1_1primitive_1a2c112f2449a18a87310dee2ecd8c64eb"><span class="std std-ref">primitive::execute()</span></a> call. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Zero points correspondence mask that defines the correspondence between the output tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">zero_points</span></code> vector. The set i-th bit indicates that a dedicated zero point is used for each index along that dimension. Set the mask to 0 to use a common zero point for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">zero_points</span></code>: Output vector of zero points. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr15set_zero_pointsEiiRKNSt6vectorI7int32_tEE">
<span id="_CPPv3N4dnnl14primitive_attr15set_zero_pointsEiiRKNSt6vectorI7int32_tEE"></span><span id="_CPPv2N4dnnl14primitive_attr15set_zero_pointsEiiRKNSt6vectorI7int32_tEE"></span><span id="dnnl::primitive_attr::set_zero_points__i.i.std::vector:int32_t:CR"></span><span class="target" id="structdnnl_1_1primitive__attr_1aee82deb014cf9702ceb3e725156c25a1"></span>void <code class="sig-name descname">set_zero_points</code><span class="sig-paren">(</span>int <em>arg</em>, int <em>mask</em>, <em class="property">const</em> std::vector&lt;int32_t&gt; &amp;<em>zero_points</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr15set_zero_pointsEiiRKNSt6vectorI7int32_tEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets zero points for primitive operations for a given memory argument.</p>
<p><dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference internal" href="#structdnnl_1_1primitive__attr_1a4b81acc8e48886313154f75c1708ae02"><span class="std std-ref">dnnl::primitive_attr::set_output_scales</span></a></p>
</dd>
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">arg</span></code>: Parameter argument index as passed to the <a class="reference internal" href="../general.html#structdnnl_1_1primitive_1a2c112f2449a18a87310dee2ecd8c64eb"><span class="std std-ref">primitive::execute()</span></a> call. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Zero point correspondence mask that defines the correspondence between the tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">zero_points</span></code> vector. The set i-th bit indicates that a dedicated zero point is used for each index along that dimension. Set the mask to 0 to use a common zero point for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">zero_points</span></code>: Constant vector of zero points. If the zero points are known at the time of this call, the following equality must hold: <span class="math notranslate nohighlight">\(zero\_points.size() = \prod\limits_{d \in mask} argument.dims[d].\)</span> If the zero points are not known at the time of the call, this vector must contain a single <a class="reference internal" href="../general.html#dnnl__types_8h_1ab16365c11b4dc88fbb453edb51f1979f"><span class="std std-ref">DNNL_RUNTIME_F32_VAL</span></a> value and the zero points must be passed at execution time as an argument with index <a class="reference internal" href="../general.html#dnnl__types_8h_1af8d879adfe2baa2f9f2a5143a0f274b6"><span class="std std-ref">DNNL_ARG_ATTR_ZERO_POINTS</span></a>. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4NK4dnnl14primitive_attr12get_post_opsEv">
<span id="_CPPv3NK4dnnl14primitive_attr12get_post_opsEv"></span><span id="_CPPv2NK4dnnl14primitive_attr12get_post_opsEv"></span><span id="dnnl::primitive_attr::get_post_opsC"></span><span class="target" id="structdnnl_1_1primitive__attr_1a05664ef63c94acbcc59e921c4a4da6b8"></span><em class="property">const</em> <a class="reference internal" href="post-ops.html#_CPPv4N4dnnl8post_opsE" title="dnnl::post_ops">post_ops</a> <code class="sig-name descname">get_post_ops</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em><a class="headerlink" href="#_CPPv4NK4dnnl14primitive_attr12get_post_opsEv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Returns post-ops previously set via <a class="reference internal" href="#structdnnl_1_1primitive__attr_1ac830fa9f4fcf480b494d73153ad579bf"><span class="std std-ref">set_post_ops()</span></a>.</p>
<p><dl class="simple">
<dt><strong>Return</strong></dt><dd><p>Post-ops. </p>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr12set_post_opsEK8post_ops">
<span id="_CPPv3N4dnnl14primitive_attr12set_post_opsEK8post_ops"></span><span id="_CPPv2N4dnnl14primitive_attr12set_post_opsEK8post_ops"></span><span id="dnnl::primitive_attr::set_post_ops__post_opsC"></span><span class="target" id="structdnnl_1_1primitive__attr_1ac830fa9f4fcf480b494d73153ad579bf"></span>void <code class="sig-name descname">set_post_ops</code><span class="sig-paren">(</span><em class="property">const</em> <a class="reference internal" href="post-ops.html#_CPPv4N4dnnl8post_opsE" title="dnnl::post_ops">post_ops</a> <em>ops</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr12set_post_opsEK8post_ops" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets post-ops.</p>
<p><dl class="simple">
<dt><strong>Note</strong></dt><dd><p>There is no way to check whether the post-ops would be supported by the target primitive. Any error will be reported by the respective primitive descriptor constructor.</p>
</dd>
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ops</span></code>: Post-ops object to copy post-ops from. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr20set_rnn_data_qparamsEff">
<span id="_CPPv3N4dnnl14primitive_attr20set_rnn_data_qparamsEff"></span><span id="_CPPv2N4dnnl14primitive_attr20set_rnn_data_qparamsEff"></span><span id="dnnl::primitive_attr::set_rnn_data_qparams__float.float"></span><span class="target" id="structdnnl_1_1primitive__attr_1a39ce5aa8b06ed331d8e2158108cc8324"></span>void <code class="sig-name descname">set_rnn_data_qparams</code><span class="sig-paren">(</span>float <em>scale</em>, float <em>shift</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr20set_rnn_data_qparamsEff" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets quantization scale and shift parameters for RNN data tensors.</p>
<p>For performance reasons, the low-precision configuration of the RNN primitives expect input activations to have the unsigned 8-bit integer data type. The scale and shift parameters are used to quantize floating-point data to unsigned integer and must be passed to the RNN primitive using attributes.</p>
<p>The quantization formula is <code class="docutils literal notranslate"><span class="pre">scale</span> <span class="pre">*</span> <span class="pre">(data</span> <span class="pre">+</span> <span class="pre">shift)</span></code>.</p>
<p>Example usage: <div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">RNN</span> <span class="n">parameters</span>
<span class="nb">int</span> <span class="n">l</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">mb</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">sic</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">slc</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">dic</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">dlc</span> <span class="o">=</span> <span class="mi">32</span><span class="p">;</span>
<span class="o">//</span> <span class="n">Activations</span> <span class="n">quantization</span> <span class="n">parameters</span>
<span class="nb">float</span> <span class="n">scale</span> <span class="o">=</span> <span class="mf">2.0</span><span class="n">f</span><span class="p">,</span> <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.5</span><span class="n">f</span><span class="p">;</span>

<span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>

<span class="o">//</span> <span class="n">Set</span> <span class="n">scale</span> <span class="ow">and</span> <span class="n">shift</span> <span class="k">for</span> <span class="n">int8</span> <span class="n">quantization</span> <span class="n">of</span> <span class="n">activation</span>
<span class="n">attr</span><span class="o">.</span><span class="n">set_rnn_data_qparams</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">);</span>

<span class="o">//</span> <span class="n">Create</span> <span class="ow">and</span> <span class="n">configure</span> <span class="n">rnn</span> <span class="n">op_desc</span>
<span class="n">vanilla_rnn_forward</span><span class="p">::</span><span class="n">desc</span> <span class="n">rnn_d</span><span class="p">(</span><span class="o">/*</span> <span class="n">arguments</span> <span class="o">*/</span><span class="p">);</span>
<span class="n">vanilla_rnn_forward</span><span class="p">::</span><span class="n">primitive_desc</span> <span class="n">rnn_d</span><span class="p">(</span><span class="n">rnn_d</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">engine</span><span class="p">);</span>
</pre></div>
</div>
</p>
<p><dl class="simple">
<dt><strong>Note</strong></dt><dd><p>Quantization scale and shift are common for src_layer, src_iter, dst_iter, and dst_layer.</p>
</dd>
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scale</span></code>: The value to scale the data by. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shift</span></code>: The value to shift the data by. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="cpp function">
<dt id="_CPPv4N4dnnl14primitive_attr23set_rnn_weights_qparamsEiRKNSt6vectorIfEE">
<span id="_CPPv3N4dnnl14primitive_attr23set_rnn_weights_qparamsEiRKNSt6vectorIfEE"></span><span id="_CPPv2N4dnnl14primitive_attr23set_rnn_weights_qparamsEiRKNSt6vectorIfEE"></span><span id="dnnl::primitive_attr::set_rnn_weights_qparams__i.std::vector:float:CR"></span><span class="target" id="structdnnl_1_1primitive__attr_1a61bd70f97baa628fd49b2c8b334b913e"></span>void <code class="sig-name descname">set_rnn_weights_qparams</code><span class="sig-paren">(</span>int <em>mask</em>, <em class="property">const</em> std::vector&lt;float&gt; &amp;<em>scales</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4dnnl14primitive_attr23set_rnn_weights_qparamsEiRKNSt6vectorIfEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets quantization scaling factors for RNN weights tensors. The low-precision configuration of the RNN primitives expect input weights to use the signed 8-bit integer data type. The scaling factors are used to quantize floating-point data to signed integer and must be passed to RNN primitives using attributes.</p>
<p><dl class="simple">
<dt><strong>Note</strong></dt><dd><p>The dimension order is always native and does not depend on the actual layout used. For example, five-dimensional weights always have (l, d, i, g, o) logical dimension ordering.</p>
</dd>
<dt><strong>Note</strong></dt><dd><p>Quantization scales are common for weights_layer and weights_iteration</p>
</dd>
<dt><strong>Parameters</strong></dt><dd><ul class="breatheparameterlist simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Scaling factors correspondence mask that defines the correspondence between the output tensor dimensions and the <code class="docutils literal notranslate"><span class="pre">scales</span></code> vector. The set i-th bit indicates that a dedicated scaling factor should be used each index along that dimension. Set the mask to 0 to use a common scaling factor for the whole output tensor. </p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scales</span></code>: Constant vector of output scaling factors. The following equality must hold: <span class="math notranslate nohighlight">\(scales.size() = \prod\limits_{d \in mask} weights.dims[d].\)</span> Violations can only be detected when the attributes are used to create a primitive descriptor. </p></li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="post-ops.html" class="btn btn-neutral float-right" title="Post-ops" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../general.html" class="btn btn-neutral float-left" title="Common Definitions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Intel Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>